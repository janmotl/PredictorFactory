<?xml version="1.0" encoding="utf-8" standalone="yes"?>

<pattern>
	<name>Entropy</name>
	<description>
		Shannon entropy for discrete distributions.

		The returned entropy is by definition non-negative and is maximized by uniform distribution. The entropy
		(generally) increases with the increasing count of unique values.

		This pattern is slow.

		Note: To avoid integer division, we multiply the ratio by 1.0 to cast it to numerical.
		Note: The base of the logarithm is database vendor specific.
		Note: This is a population estimate. Not sample estimate. See:
			http://thirdorderscientist.org/homoclinic-orbit/2013/7/9/how-many-ways-can-you-estimate-entropy
		Note: For continuous variables, see Approximate entropy:
			https://en.wikipedia.org/wiki/Approximate_entropy
	</description>
	<example>

	</example>
	<author>Jan Motl</author>
	<date>2017-02-02</date>
	<code compatibility="MySQL,PostgreSQL">
		SELECT @base, -sum(prob*log(prob)) AS @columnName
		FROM (
			SELECT @base, @nominalColumn, 1.0*item_cnt.cnt/global_cnt.cnt as prob
			FROM (
				SELECT @base, @nominalColumn
					 , count(*) cnt
				FROM @propagatedTable
				GROUP  BY @base, @nominalColumn
			) item_cnt
			JOIN (
				SELECT @base, count(*) as cnt
				FROM @propagatedTable
				GROUP BY @base
			) global_cnt
			USING(@base)
		) prob
		GROUP BY @base
	</code>
	<cardinality>n</cardinality>
</pattern>
