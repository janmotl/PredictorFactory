<?xml version="1.0" encoding="utf-8" standalone="yes"?>

<pattern>
	<name>WOE</name>
	<description>Weight of evidence with leave-one-out.

		WoE = 100 * ln( (#positive_i / #positive) / (#negative_i / #negative) )

		In comparison to the textbook formulation this implementation takes care of:
			1) Missing values (they are separately treated by isNull pattern)
			2) Zero probabilities (with Laplace correction)
			3) Polynomial target
			4) Leaking target values (by using leave one out trick - it is fast as we just subtract the current target value from the global results)

		For review of advantages and disadvantages of WoE see:
			https://stats.stackexchange.com/questions/189568/replacing-variables-by-woe-weight-of-evidence-in-logistic-regression
		In practise, WoE is interesting for nominal attributes with high cardinality (high count of unique values).

		The implementation is from: https://qizeresearch.wordpress.com/2014/05/21/utilize-woe-to-replace-dummy-variables/

		1) We do not calculate WoE for NULL category in the attribute. This is intentional as we leave the treatment of
		nulls on isNull pattern. The reasoning follows:
			1) If all patterns (but isNull and nullRatio) can ignore nulls in the attributes, it simplifies their
			design (e.g. we do not have to deal with joining over nullable attributes).
			2) If the nulls are missing completely at random (MCAR), nulls are not predictive.
			3) But if the nulls are predictive, their predictivity is explained with a single predictor. The
			predictivity does not leak into all other predictors. Hence, if only presence of nulls was predictive and
			not the actual values, only isNull is going to be predictive and all remaining predictors will be correctly
			treated as uninformative.
			4) It decreases correlation of the predictors.
			5) It decreases runtime and memory consumption of the predictors as only non-null values are processed and
			stored (isNull and nullRatio are dedicated exceptions).

		2) To avoid division by zero and logarithm of zero in evaluation of WoE we use the Laplace correction.
		Laplacean correction is from: http://shigglesblog.blogspot.com/2013/07/weight-of-evidence.html

		A different correction schema:
			http://support.sas.com/documentation/cdl/en/prochp/66704/HTML/default/viewer.htm#prochp_hpbin_details02.htm
		Based on the empirical comparison performed by Maros Spak, the implemented correction schema performs on average
		by 0.5% better than the correction implemented by SAS. This difference was consistent across
		different models (decision tree, logistic regression, naive Bayes), different metrics (classification accuracy,
		f-measure, AUC) and different datasets (7 datasets). From the theoretical point of view, the implemented
		correction produces scores that can be interpreted as a probability (e.g. they sum to 1), while the SAS score
		cannot be interpreted as a probability because the scores do not sum to 1 (at least for finite count of samples).

		The empirical evidence suggests that Laplacean correction improves AUC by 5 percent points.
			See: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1918410

		We have also tested m-estimate, which does not assume uniform distribution of the label, like Laplace correction.
		But empirically it performed worse on the testing set than Laplace correction.

		3) Multi-class WoE: http://dspace.library.uu.nl/bitstream/handle/1874/11641/c4.pdf

		4) The prior counts (#positive and #negative) are intentionally not adjusted by leave-one-out calculation. Let's
		take a trivial example, where we have one id-like attribute 'att' and a binary label 'label':
			att label 	#positive_i		#negative_i		#positive	#negative	WoE_with_Laplace_correction
			1	p 		0				0				2 (1)		2 (2)		0 (28.8)
			2	n 		0				0				2 (2)		2 (1)		0 (-28.8)
			3	p 		0				0				2 (1)		2 (2)		0 (28.8)
			4	n 		0				0				2 (2)		2 (2)		0 (-28.8)
		The remaining columns are computed based on 'att' and 'label', where values in the brackets are after LOO
		adjustment (ignoring the row). Arguably, id-like attributes that do not repeat should have zero predictive
		power on the testing set. And if we LOO adjust only counts for the att_i value, we get that. But if we also
		adjust the prior, the label leaks into the predictor (see values in the brackets in the last column).

		If LOO leaks information, what about n-fold validation? Let's consider 2-fold validation where we calculate
		everything at fold level (see values in the brackets).
			att label 	fold 	#positive_i		#negative_i		#positive	#negative	WoE_with_Laplace_correction
			1	p 		1		1				1				1			1			0
			2	n 		1		1				1				1			1			0
			3	p 		2		1				1				1			1			0
			4	n 		2		1				1 				1			1			0
		It works!

		But what if we have unbalanced folds?
			att label 	fold 	#positive_i		#negative_i		#positive	#negative	WoE_with_Laplace_correction
			1	p 		1		1				1				1			1			0
			2	n 		1		1				1				1			1			0
			3	p 		2		2 				1				2			1			11.8
			4	n 		2		2 				1				2			1			11.8
			5	p 		2		2 				1				2			1			11.8
		Not the best, because the assignment of the rows into the folds influences the predictive power of the predictor.
		The confusion matrix for calculation Chi2 is:
			 		p 	n
			0		1	1
			11.8	2	1
		Chi2 is: 0.139 (not a zero as we would like).

		How LOO works on unbalanced labels (if we do not apply LOO on the prior)?
			att label 	#positive_i		#negative_i		#positive	#negative	WoE_with_Laplace_correction
			1	p 		0				0				3			2			-22.3
			2	n 		0				0				3			2			-22.3
			3	p 		0 				0				3			2			-22.3
			4	n 		0 				0				3			2			-22.3
			5	p 		0 				0				3			2			-22.3
		While it does not return 0, because the priors are unbalanced, the predictor itself is not predictive as it is
		constant. And that is what do we need.

		Conclusion: use LOO.

		Implementation details: the prior counts can be obtained with @targetValueCount. That way we can avoid repeated
		calculation of the prior. But for clarity, we do not do that. Similarly, the outer-most wrapping in the subquery
		is only for clarity.


		Note that WoE still ignores confidence of the estimates. For example, if we have 2 positive samples and
		5 negative samples, we get the same WoE as if we had 2000 positive samples and 5000 negative samples.
		Intuitively, the second scenario should result into a value further from zero. Nevertheless, this problem is
		partially solved with leave-one-out: with small sample size, the variance of the estimate is higher.


		Target value is always treated as a string because databases automatically cast numbers to strings, if necessary.
		But databases do not automatically cast strings to numbers. Hence, treating the target value as a string works
		for both, numerical and string targets.
	</description>
	<note>
		Note: We use {fn log(x) } because Oracle 11g EX expects presence of the base in the logarithm.
		Hence we can't universally use it.
		Another story is ln(x), which is supported by Oracle and MySQL but not by MSSQL. In MSSQL it is log(x).


		Note: LOO works at row level -> if we have records about the same person over time, the label from the past
		is going to leak into the predictor. While it would be MUCH nicer to have a dedicated pattern that would look
		at the past values of the label,

		NOTE: WORKS ONLY ON BINARY TARGETS! -> produce a column for each target class

		NOTE: Is expected to fail on regression tasks.
	</note>
	<author>Jan Motl</author>
	<date>2016-06-17</date>
	<code>SELECT @base
				, 100 * {fn log( (pos_i_loo/pos_all) / (neg_i_loo/neg_all) )} as @columnName
				FROM (
					SELECT t1.@base
						, t1.@nominalColumn
						, pos_i - (case when @baseTarget  = '@targetValue' then 1.0 else 0 end) as pos_i_loo
						, neg_i - (case when @baseTarget &lt;&gt; '@targetValue' then 1.0 else 0 end) as neg_i_loo
						, pos_all
						, neg_all
					FROM @propagatedTable t1
					JOIN (
						SELECT @nominalColumn
							, pos_all
							, neg_all
							, (sum(case when @baseTarget  = '@targetValue' then 1.0 else 0 end)+1) as pos_i
							, (sum(case when @baseTarget &lt;&gt; '@targetValue' then 1.0 else 0 end)+1) as neg_i
						FROM @propagatedTable
							, (
								SELECT sum(case when @baseTarget ='@targetValue' then 1.0 else 0 end)+2 as pos_all
									, sum(case when @baseTarget &lt;&gt; '@targetValue' then 1.0 else 0 end)+2 as neg_all
								FROM @propagatedTable
							) total
						GROUP BY @nominalColumn, pos_all, neg_all
					) raw
					ON raw.@nominalColumn = t1.@nominalColumn
				) leave_one_out
	</code>
	<cardinality>1</cardinality>
</pattern>
